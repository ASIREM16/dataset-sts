Answer Sentence Selection (YodaQA)
==================================

In Question Answering systems on top of unstructured corpora, one task is
selecting the sentences in corpora that are most likely to carry an answer
to a given question.  In this scenario, the questions come from the
``curated-train`` YodaQA dataset and the YodaQA system generated the
candidate sentences based on enwiki, using YodaQA f/sentence-selection
branch commit a5f7f98 (based on v1.5).

Sentences were generated by running fulltext solr search on enwiki for
keywords extracted from the question, and then considering all sentences
from top N results that contain at least a single such keyword.  Sentences
that match the gold standard answer regex are labelled as 1, the rest is 0.
This *automatic labelling* means the dataset is **quite noisy**.

The key metric here is MRR when ranking sentences answering the same question
by their score, but raw accuracy may be also interesting.  The dataset is
heavily unbalanced!  In our models, we subsample 0-entries as well as
supersample 1-entries.  First 80% entries result in the training set, last 20%
is validation set; if you need a dev set (e.g. for early stopping), use last 5%
of training set.

This dataset is subject to change and evolution, hence the ``v1``.  (Most
importantly, we want to let Turkers update the gold standard answer regex.)

Older Datasets
--------------

The academic standard up to now stems from **anssel-wang** - the TREC-based
dataset originally by Wang et al., 2007, in the form by Yao et al., 2013.
However, this dataset seems to be quite easy as the ratio of relevant snippets
is high and they are often pretty short.

That's why we are introducing a new one, which is also somewhat bigger.

See also the WikiQA corpus, which has too restrictive licence but manually
labelled pairs.

Licence
-------

Derived from the ``curated-train`` YodaQA dataset

	htts://github.com/brmson/dataset-factoid-curated

and sentences from Wikipedia, Wikipedia is CC-BY-SA if that's relevant.
