Answer Sentence Selection (YodaQA)
==================================

In Question Answering systems on top of unstructured corpora, one task is
selecting the sentences in corpora that are most likely to carry an answer
to a given question.  In this scenario, the questions come from the
``curated-train`` YodaQA dataset and the YodaQA system generated the
candidate sentences based on enwiki, using YodaQA f/sentence-selection
branch commit a5f7f98 (based on v1.5).

Sentences were generated by running fulltext solr search on enwiki for
keywords extracted from the question, and then considering all sentences
from top N results that contain at least a single such keyword.  Sentences
that match the gold standard answer regex are labelled as 1, the rest is 0.
This *automatic labelling* means the dataset is **quite noisy**.

The key metric here is MRR when ranking sentences answering the same question
by their score, but raw accuracy may be also interesting.  The dataset is
heavily unbalanced!  In our models, we subsample 0-entries as well as
supersample 1-entries.  First 80% entries result in the training set, last 20%
is validation set; if you need a dev set (e.g. for early stopping), use last 5%
of training set.

This dataset is subject to change and evolution, hence the ``v1``.  (Most
importantly, we want to let Turkers update the gold standard answer regex.)

Older Datasets
--------------

The academic standard up to now stems from the TREC-based dataset originally
by Wang et al., 2007, in the form by Yao et al., 2013 as downloaded from
https://code.google.com/p/jacana/ - however, it suffers from a variety of
ailments; it's hard to make sense of and process, it's not very high quality,
and most importantly the train/test split is very unbalanced, both in the
kind of sentence pairs and difficulty (train set is a lot harder!).

That's why we are introducing a new one.

See also the WikiQA corpus, which has too restrictive licence but manually
labelled pairs.
